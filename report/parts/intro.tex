\documentclass[../report.tex]{subfiles}

\begin{document}

A mean-field game \cite{LASRY2006619,LASRY2006679} is a strategic decision-making problem with a very large, continuously-distributed number of interacting agents inside a state space: the overall theory developed by \citeauthor{LASRY2006619} can be used as a means to model large, computationally intractable games. Each agent's actions get a feedback response that depends on other agents' states and actions through a \textit{mean-field} effect.

In a dynamic MFG, every agent penalizes a running cost as well as a terminal cost on the its final position and the overall final distribution of agents \cite{LASRY2006679}.
The framework of \cites{benamou:hal-01295299}{benamou2018entropy} focuses on games with stochastic dynamics
\begin{equation}\label{eq:AgentDynamics}
	dX_t = \alpha_tdt + \sigma dW_t
\end{equation}
with viscosity $\sigma$ and control $\alpha$ and a quadratic running cost on the control $L(\alpha_t) = |\alpha_t|^2/2$. The penalties are given by $L^2$-valued functionals $f$ and $g$. A representative agent's objective is to minimize the overall cost
\begin{equation}\label{eq:ControlObjective}
	\inf_{\alpha} J(\alpha) = \EE\left[
	\int_0^T \frac{1}{2}|\alpha_t|^2 + f(X_t, \rho_t) \,dt + g(X_T, \rho_T)
	\right]
\end{equation}
subject to \cref{eq:AgentDynamics}, and where $\rho_t$ is the distribution of agents at time $t$. Following the usual dynamic programming framework, we consider \textit{Markov} or \textit{closed-loop feedback} controls $\alpha_t = \phi(t, X_t)$. For quadratic running costs, $\phi = \nabla u$ where $u$ is the value function.

Nash equilibria can be summarized by the following system of coupled Hamilton-Jacobi-Bellman and diffusion (Fokker-Planck) PDEs:
\begin{subequations}\label{eq:QuadraticMFG}
	\begin{align}\label{eq:MFGHJB}
	-\partial_t u - \frac{\sigma^2}{2}\Delta u + \frac12|\nabla u|^2 &= f(x, \rho_t), \quad (t,x) \in  (0, T) \times \Omega \\\label{eq:MFGKolmo}
	\partial_t \rho_t - \frac{\sigma^2}{2}\Delta\rho_t - \divg(\rho_t \nabla u) &= 0 \\
	u(T, \cdot) &= g(x, \rho_T)
	\end{align}
\end{subequations}
where $t\mapsto \rho_t$ is a trajectory in the space of measures and $\rho_0$ is given, and $\Omega\subset\RR^d$. The maps $f(\cdot, \mu)$ and $g(\cdot, \mu)$ are supposed to be derivatives of some real-valued functionals $F$ and $G$. For instance, if the terminal cost is a function of space $g(x, \mu) = \Psi(x)$, it has antiderivative $G(\mu) = \int_\Omega \Psi(x) \,d\mu(x)$.
These PDEs can be solved in some cases using finite-difference schemes \cite{achdou:hal-01456506}.

\end{document}