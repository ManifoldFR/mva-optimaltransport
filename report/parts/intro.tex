\documentclass[../report.tex]{subfiles}

\begin{document}

A mean-field game \cite{LASRY2006619,LASRY2006679} is a strategic decision-making problem with a very large, continuously-distributed number of interacting agents inside a state space: the overall theory developed by \citeauthor{LASRY2006619} can be used as a means to model large, computationally intractable games. Each agent's actions get a feedback response that depends on other agents' states and actions through a \textit{mean-field} effect. In the time-evolving setting, every agent obeys to some dynamics and his actions are modeled by a dynamic control problem \cite{LASRY2006679}.

The general setup of a dynamic MFG has every agent penalize a running cost on the control, aspects of the trajectory (namely the mean-field interactions with other agents), as well as a terminal cost on the its final position and the overall final distribution of agents \cite{LASRY2006679}.
The framework of \cites{benamou:hal-01295299}{benamou2018entropy} focuses on games with stochastic agent dynamics 
\begin{equation}\label{eq:AgentDynamics}
dX_t = \alpha_tdt + \sigma dW_t
\end{equation}
with viscosity $\sigma$ and control $\alpha$ and a quadratic running cost on the control $L(\alpha_t) = |\alpha_t|^2/2$. Mean-field interaction penalties are given by $L^2$-valued functionals $f$ and $g$. A representative agent's objective is to minimize the overall penalty
\begin{equation}
\inf_{\alpha} J(\alpha) = \EE\left[
\int_0^T \frac{1}{2}|\alpha_t|^2 + f(X_t, \rho_t) \,dt
+ g(X_T, \rho_T)
\right]
\end{equation}
subject to \cref{eq:AgentDynamics}, and where $\rho_t$ is the overall distribution of agents at time $t$. In the usual dynamic programming framework, we consider Markov \textbf{closed-loop feedback form} controls
\begin{equation}
	\alpha = \phi(t, X_t).
\end{equation}
In the quadratic cost framework the feedback is the gradient of the value function $\phi = \nabla u$.

The Nash equilibrium agent-control dynamics can be summarized by the partial differential equations:
\begin{subequations}\label{eq:VariationalQuadraticMFG}
	\begin{align}\label{eq:VarQuadMFGHJB}
	-\partial_t u - \frac{\sigma^2}{2}\Delta u + \frac12|\nabla u|^2 &= f(x, \rho_t), \quad (t,x) \in  (0, T) \times \Omega \\\label{eq:VarQuadMFGKolmo}
	\partial_t \rho_t - \frac{\sigma^2}{2}\Delta\rho_t - \divg(\rho_t \nabla u) &= 0 \\
	u(T, \cdot) &= g(x, \rho_T)
	\end{align}
\end{subequations}
where $t\mapsto \rho_t$ is a trajectory in the space of measures and $\rho_0$ is given, and $\Omega\subset\RR^d$. The applications $\mu\mapsto f(\cdot, \mu)$ and $\mu\mapsto g(\cdot, \mu)$ are supposed to be derivatives of some real-valued functionals $F$ and $G$ on the space of measures. For instance, in the case considered by \textcite{benamou:hal-01295299}, the running cost functional is a function of space $f(x, \mu) = \Psi(x)$, which has antiderivative $F(\mu) = \int_\Omega \Psi(x) \,d\mu(x)$ in the space of measures.

\Cref{eq:VarQuadMFGHJB,eq:VarQuadMFGKolmo} form a coupled system of control (Hamilton-Jacobi-Bellman) and diffusion (Fokker-Planck) partial differential equations. They can be solved in some cases using finite-difference methods (see \textcite{achdou:hal-01456506}).

\end{document}