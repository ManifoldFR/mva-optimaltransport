\documentclass[../report.tex]{subfiles}

\begin{document}

\subsection{Time discretization}

Let $N$ be the number of discrete steps for the time discretization of the problem, and $h=T/N$ the time step.

\textcite{benamou2018entropy} propose a discretization of \eqref{eq:EntropyLagrangianProblem} obtained by connecting the marginals through a multimarginal OT problem:
\begin{equation}
\calS(\mu_0,\ldots,\mu_N) =
\inf_{\gamma \in \Pi(\mu_0, \ldots, \mu_N)}
H(\gamma|R^N)
\end{equation}
where $t_k = kh$, $R^N = R_{t_0,\ldots,t_N}$ and the marginals $\mu_k \in \calP_2(\Omega)$.
Then, define
\begin{equation}
\mathcal{U}(\mu_0,\ldots,\mu_N) = h\sum_{k=1}^{N-1} F(\mu_k) + G(\mu_N).
\end{equation}

Thus, the discretized entropy minimization problem can be written as
\[
\inf\left\{
\calS(\mu_0, \ldots, \mu_N) +
\mathcal{U}(\mu_0, \ldots, \mu_N)
: \mu_k \in \calP_2(\Omega),\; \mu_0 = \rho_0
\right\}.
\]
Expanding the inf-within-inf leads to the following convex optimization problem:
\begin{equation}
\begin{aligned}
&\inf_{\gamma \in \calP(\Omega^{N+1})}
H(\gamma | R^N) + \imath_{\rho_0}(\mu_0) + \sum_{k=1}^{N-1} F(\mu_k) + G(\mu_N) \\
&\suchthat\ \mu_k = P^k_\#\gamma
\end{aligned}
\end{equation}
where $\imath_{\rho_0}(\mu) = +\infty$ if $\mu\neq \rho_0$ and $0$ otherwise is the convex indicatrix of the measure $\rho_0$. This is a generalized multimarginal optimal transport problem.

\textcite{benamou2018entropy} provide the corresponding dual problem involving the convex conjugates and potential functions, by using a multimarginal generalization of a result from \textcite{chizat2016scaling}:
\begin{equation}\label{eq:TimeDiscreteDual}
\sup_u
\int_{\Omega^{N+1}} \left(1-\exp\left(\oplus_{k=0}^N u_k\right)\right) \,dR^N
-\imath_{\rho_0}^*(-u_0) - \sum_{k=1}^{N-1} F^*(-u_k) - G^*(-u_N)
\end{equation}
where the supremum is taken over $u = (u_0,\ldots,u_N) \in L^\infty(\Omega)^{N+1}$.


\textcite{benamou2018entropy} introduce a Sinkhorn-like iterative algorithm to solve the above dual problem. We rewrite it more explicitly with slightly different notations inspired by \cite{chizat2016scaling}.

\begin{prop}\label{algo:Algo1}
	Denote for $k=0,\ldots,N$ and $(a_j)_{j\neq k}$
	\[
	\calI_k((a_j)_{j\neq k})(\tilde{x}_k) = 
	\int_{\Omega^N}
	\prod_{j\neq k} a_j(x_j)
	R^N(dx_{0:k-1}, \tilde{x}_k, dx_{k+1:N})
	\]
	the partial integral of the $a_j,j\neq k$ with respect to $R^N$ without variable $x_k$.
	For convenience we use the shorthand
	\[
	\calI_k^{(n)} = \calI_k\left(\left(a^{(n+1)}_j\right)_{j<k},
	\left(a^{(n)}_j\right)_{j>k}\right)
	\]
	for the $n$th iterate where we denote $a_j = \exp(u_j)$.
	
	Then we compute the dual potentials iteratively:
	\begin{equation}
	\begin{dcases}
	u_0^{(n+1)} = \argmax_{v \in L^\infty} \int_{\Omega} (1 - e^{v(x_0)}) \calI_0^{(n)} \,dx_0 - \imath_{\rho_0}^*(-v) \\
	u_k^{(n+1)} = \argmax_{v \in L^\infty} \int_{\Omega} (1- e^{v(x_k)}) \calI_k^{(n)} \,dx_k - hF^*(-v) ,
	\quad 1\leq k < N  \\
	u_N^{(n+1)} = \argmax_{v \in L^\infty} \int_{\Omega} (1 - e^{v(x_N)}) \calI_N^{(n)} \,dx_N - G^*(-v)
	\end{dcases}
	\end{equation}
	until convergence.
	
	Using duality, we find that the iterates $u_k^{(n)}$ satisfy
	\begin{equation}
	{\bluefont
	a_k^{(n)} = \exp\left(u^{(n)}_k\right) =
	\frac{
		\prox_{F_k}^{\KL}(\calI_k^{(n)})
	}{\calI_k^{(n)}}}
	\end{equation}
	where
	\[
		\prox_F^{\KL}(z) = \argmin_{s} F(s) + \KL(s|z)
	\]
	is the KL-proximal operator.
\end{prop}

\begin{remark}[Some convex conjugates]\label{rem:ConvexConj}
	In practice, the convex conjugates of the cost functions are difficult to compute. For some of the examples in the paper, we have closed-form conjugates.
	\begin{itemize}
		\item The conjugate of the convex indicatrix $\imath_{\nu}$ of a measure $\nu$ is given by $\imath_{\nu}^*(u) = \langle u, \nu\rangle$.
		\item The hard congestion constraint
		\[
		C(\rho) = \begin{cases}
		0&\text{ if }\rho\leq \bar{m} \\
		+\infty&\text{ otherwise}
		\end{cases}
		\]
		has convex conjugate (on the domain $\rho \geq 0$)
		\[
		C^*(u) = \sup_{\rho\leq \bar{m}}{} \langle u, \rho\rangle = \langle u^{+}, \bar{m}\mathds{1}\rangle
		\]
		\item Obstacle constraints, given by
		\[
		F(\rho) = \int_\Omega V(x)\,d\rho(x) =
		\begin{cases}
		0 & \mbox{ if } \rho = 0\text{ on }\mathscr{O} \\
		+\infty & \mbox{ otherwise}
		\end{cases}
		= \imath_{0}(\mathds{1}_{\mathscr{O}}\rho)
		\]
		where $V$ is the convex indicatrix of the complement $\Omega\backslash\mathscr{O}$ of the obstacles. Its conjugate is given by
		\[
		F^*(u) =
		\begin{cases}
		0& \mbox{if } u \leq 0\text{ on } \Omega\backslash\mathscr{O} \\
		+\infty& \mbox{otherwise}
		\end{cases}
		\]
	\end{itemize}
\end{remark}



\subsection{Spatial discretization}\label{sec:PartialDiscret}

For full numerical implementation, all measures are replaced by multi-dimensional arrays representing discrete histograms over a fixed grid of points $x_i$ in $\RR^d$ of size $M = N_1\times\cdots\times N_d$.

Integration with respect to the marginalized Wiener measure $R^N$ is the main computational bottleneck. 

Denote $\bfR \in \RR^{M^N}$ the discretized measure $R^N$. Integration of multiple vectors $a_0,\ldots,a_N\in\RR^M$ with respect to $\bfR$ is the following tensor contraction
\[
	\bfR[a_0, \ldots, a_N] =
	\sum_{i_0,\ldots,i_N} \bfR_{i_0,\ldots,i_N}\prod_{k=0}^N a_{i_k}
\]
A naive implementation would compute the sum in exponential time $\calO(NM^N)$: this is a well known problem is computational statistics and graphical models, and an efficient way of dealing with it is exploiting the structure of the kernel $\bfR$.

\begin{prop}[Efficient convolution]\label{prop:efficientConvol}
The kernel $\bfR$ can be factorized as $\bfR_{i_0,\ldots,i_N} = \prod_{k=0}^{N-1} \bfP_{i_k,i_{k+1}}$ where $\bfP$ is the discrete heat kernel on $\RR^M$. The partial convolution $\calI_k$ (leaving the $k$th component out) can now be written as
\begin{equation}
{\bluefont
	\calI_k = \bfR[(a_j)_{j\neq k}] =
	\mathbf{A}_{k-1} \odot \mathbf{B}_{k+1}
}
\end{equation}
where $\mathbf{A}_{k} = \bfP^T (a_k \odot \bfP^T(a_{k-1} \odot \cdots))$ and $\mathbf{B}_k = \bfP (a_k \odot \bfP (a_{k+1}\odot \cdots))$.
\end{prop}

Due to the structure of the kernel, the marginals $\mu_0,\ldots,\mu_N$ only communicate as in an undirected chain. This leads to the usual message-passing algorithm: see \Cref{algo:EfficientIntegral}.
\begin{algorithm}[h]
\caption{Efficient computation of the integral $\calI_k$.}\label{algo:EfficientIntegral}
\KwIn{Base heat kernel $\bfP$, index $k$, vectors $(a_j)_{j\neq k}$}
$\mathbf{A} \leftarrow \mathds{1}$\;
\For{$i=0$ to $k-1$}{
	$\mathbf{A} \leftarrow \bfP^T(a_i \odot\mathbf{A})$\;
}
$\mathbf{B} \leftarrow \mathds{1}$\;
\For{$i=N$ down to $k+1$}{
	$\mathbf{B} \leftarrow \bfP(a_i \odot\mathbf{B})$\;
}
\Return{$\mathbf{A}\odot\mathbf{B}$}\;
\end{algorithm}

The computational complexity of this algorithm depends on how efficiently we can compute the convolution $\mathbf{P}u$. The naive matrix product performs in time $\calO(M^3)$ and the overall algorithm is $\calO(NM^3)$ which can still be very high. For separable kernels, decomposing the convolution can net considerable speedups \parencite[p.~74]{peyr2018computational}.

\paragraph{Projections.}
In the general case, the KL-proximal operators in the Sinkhorn iterations can be solved using the Python library CVXPY\footnote{\url{https://github.com/cvxgrp/cvxpy}}\textsuperscript{,}\footfullcite{cvxpy}. Some can be computed explicitly:


\begin{prop}\label{prop:KLproxExamples}
	The KL-projection on the hard congestion constraint of a measure $\beta\in\RR^M$ is given by
	\begin{equation}
	\proj^{\KL}_\calC(\beta) = \min(\beta,\overline{m})
	\end{equation}
	where the minimum is taken element-wise.
	
	The KL-projection on the obstacle constraint is
	\begin{equation}
	\proj^{\KL}_\calC (\beta) = \beta \mathds{1}_{\Omega\backslash\mathscr{O}}
	\end{equation}
	
	If we also add the obstacle constraint on a set $\mathscr{O}$ of points in the grid, then the projector is
	\begin{equation}
	\proj^{\KL}_\calC(\beta) = \min(\beta, \bar{m})\mathds{1}_{\Omega\backslash\mathscr{O}}.
	\end{equation}
	
	Under a system of constraints with hard congestion and obstacles, a linear penalty function $G(\alpha) = \langle\Psi, \alpha\rangle$ has proximal operator
	\begin{equation}
	\prox^{\KL}_{G,\calC}(\beta) =
	\min(\beta\odot e^{-\Psi/\epsilon},\bar{m}) \mathds{1}_{\Omega\backslash\mathscr{O}}
	\end{equation}
\end{prop}







\end{document}